uniparser-grammar-eastern-armenian
==================================

This is a rule-based morphological analyzer for Modern Eastern Armenian. It is based on a formalized description of standard Eastern Armenian morphology, which also includes a number of oral and historical elements. The description is carried out in the UniParser format and includes a list of paradigms (paradigms.txt) and a grammatical dictionary (hye-lexemes-XXX.txt files). The dictionary contains descriptions of individual lexemes, each of which is accompanied by information about its stem, its part-of-speech tag and some other grammatical information, its inflectional type (paradigm), and, for some items, their English translation and stem gloss. The recall of the analyzer on literary texts is about 93%, e.g. 93% tokens receive at least one analysis.

This description can be used for morphological analysis of Armenian texts in the following ways:

1. The ``wordlists`` directory contains the output of the analyzer for a frequency list of tokens based on the Eastern Armenian National Corpus. The simplest solution is to use this analyzed wordlist for analyzing your texts. The lists include over 450 thousand analyzed types (unique words), as well as almost 1 million unanalyzed ones (which tend to have lower frequency though). Since the corpus is large, almost all tokens in your texts will be on that list.

2. The ``analyzer`` directory contains the UniParser set of scripts together with all necessary language files. You can use it to analyze your own frequency word list. Your have to name your list "wordlist.csv" and put it to that directory. Each line should contain one token and its frequency, tab-delimited. Frequencies are only needed to calculate recall, so if you don't need that, you could just write e.g. 1 for each token. When you run ``analyzer/UniParser/analyze.py``, the analyzer will produce two files, one with analyzed tokens, the other with unanalyzed ones. (You can also use other file names and separators with command line options, see the code of ``analyze.py``.) This way, you will not be restricted by the word list, but the analyzer works pretty slowly.

3. Finally, you are free to convert/adapt the description to whatever kind of morphological analysis you prefer to use.

Since the analyzer is rule-based and does not take the context into account, it produces all potentially valid analyses for each word, which leads to ambiguity (1.25 analyses per analyzed token, on average). Apart from the analyzer, this repository contains a set of (Constraint Grammar)[https://visl.sdu.dk/constraint_grammar.html] rules, which can be used to partially disambiguate analyzed Armenian texts.